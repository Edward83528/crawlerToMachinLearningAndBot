{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查詢自由時報關鍵字，儲存至MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "薇拉夫人的國際關係料理藝術》 川普 與普欽高峰會：美俄關係的舊問題與新挑戰\n",
      "俄國阿嬤外表激似 川普  網路爆紅\n",
      "歐盟下週與美協商汽車關稅  川普 ：結果不公將遭「巨大報復」\n",
      "川普 新關稅？ 美國對進口鈾啟「232調查」\n",
      "日企憂貿易戰 6月對美出口17個月來首跌\n",
      "川普 挺俄被罵賣國賊 FBI局長表態：俄國確實介入美國大選\n",
      "前白宮戰略顧問放話 美國會打贏貿易戰！\n",
      "金融公司齊轟 川普 貿易戰 庫德洛：要就怪中國和歐洲\n",
      "大叔男星自豪集滿體位 遇20歲鮮肉演員大解放\n",
      "不怕貿易戰？ 微信支付計劃擴大美國市場\n",
      "美國防部「有權派航母通過台海」 我重申支持公海暢行\n",
      "蕭美琴等立委在美關切台灣鋼鋁被課關稅\n",
      "貿易戰干擾 工研院下修我國製造業今年產值\n",
      "看到鬼？ 川普 妻與普廷握手後 嚇到表情變這樣...\n",
      "「虧雞福來爹」預算噤口 黃光芹驚問林義豐自我定位\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#coding:utf-8\n",
    "#65001\n",
    "import urllib.request\n",
    "import json\n",
    "import codecs\n",
    "import sys\n",
    "import argparse as ap\n",
    "import time\n",
    "import datetime\n",
    "import lxml.html\n",
    "import pymongo as mg\n",
    "from urllib.parse import quote\n",
    "\n",
    "#python main.py 世大運 2015-07-01 2017-07-03 1\n",
    "# def argParse():\n",
    "#     parser=ap.ArgumentParser(description='Liberty Time Net Crawler')\n",
    "#     parser.add_argument(\"keyword\", help=\"Serch Keyword\")\n",
    "#     parser.add_argument(\"start_date\", help=\"Start (2017-01-01)\")\n",
    "#     parser.add_argument(\"end_date\", help=\"End (2017-01-02)\")\n",
    "#     parser.add_argument(\"pages\", help=\"Pages\")\n",
    "#     return parser.parse_args()\n",
    "\n",
    "# args=argParse()\n",
    "# keyword = quote(args.keyword)\n",
    "# start_date = args.start_date\n",
    "# end_date = args.end_date\n",
    "# pages = args.pages\n",
    "\n",
    "keyword = quote('川普')\n",
    "start_date = '2018-07-01'\n",
    "end_date = '2018-07-19'\n",
    "pages = '1'\n",
    "\n",
    "client = mg.MongoClient('127.0.0.1:27017')\n",
    "db = client['ltnews']\n",
    "\n",
    "def start_requests():\n",
    "    start_list = start_date.split(\"-\")\n",
    "    end_list = end_date.split(\"-\")\n",
    "    SYear = ''\n",
    "    SMonth = ''\n",
    "    SDay = ''\n",
    "    EYear = ''\n",
    "    EMonth = ''\n",
    "    EDay = ''\n",
    "    if(len(start_list)==3) and (len(end_list)==3):\n",
    "        SYear = start_list[0]\n",
    "        SMonth = start_list[1]\n",
    "        SDay = start_list[2]\n",
    "        EYear = end_list[0]\n",
    "        EMonth = end_list[1]\n",
    "        EDay = end_list[2]\n",
    "    else:\n",
    "        print (\"Date format error.\")\n",
    "  \n",
    "    urls = []\n",
    "    for i in range(1,int(pages)+1):\n",
    "        str_idx = ''+('%s' % i)\n",
    "        api = 'http://news.ltn.com.tw/search?keyword='+keyword+'&conditions=and&SYear='+SYear+'&SMonth='+SMonth+'&SDay='+SDay+'&EYear='+EYear+'&EMonth='+EMonth+'&EDay='+EDay+'&page='+str_idx+''\n",
    "        urls.append(api)\n",
    "        for url in urls:\n",
    "            #print (url)\n",
    "            parseLtnNews(url)\n",
    "            time.sleep(0.2)\n",
    "\n",
    "\n",
    "def parseLtnNews(uri):\n",
    "    handle = urllib.request.urlopen(uri)\n",
    "    encoding = handle.headers.get_content_charset()\n",
    "    html_data =  handle.read().decode(encoding)\n",
    "    selector = lxml.html.document_fromstring(html_data)\n",
    "    newslist = selector.xpath('//*[@class=\"searchlist boxTitle\"]/li')\n",
    "    \n",
    "    for i in range(len(newslist)):\n",
    "        strTitle = ''\n",
    "        strUrl = ''\n",
    "        strBody = ''\n",
    "        strDate = ''\n",
    "        str_idx = str(i+1)\n",
    "        str_xpath = '//*[@class=\"searchlist boxTitle\"]/li['+str_idx+']/a//text()'\n",
    "        titleList = selector.xpath(str_xpath)\n",
    "        strTitle = \" \".join(titleList)\n",
    "        print(strTitle)\n",
    "        str_xpath = '//*[@class=\"searchlist boxTitle\"]/li['+str_idx+']/a//@href'\n",
    "        urlList = selector.xpath(str_xpath)[0]\n",
    "        strUrl = ''.join(urlList)\n",
    "        strUrl = strUrl\n",
    "        print(strUrl)\n",
    "        str_xpath = '//*[@class=\"searchlist boxTitle\"]/li['+str_idx+']/p//text()'\n",
    "        bodyList = selector.xpath(str_xpath)\n",
    "        strBody = ''.join(bodyList).replace('\\n','')\n",
    "        str_xpath = '//*[@class=\"searchlist boxTitle\"]/li['+str_idx+']/span//text()'\n",
    "        dateList = selector.xpath(str_xpath)\n",
    "        strDate = ''.join(dateList).replace(\"&nbsp;\",\"\")[:10]\n",
    "        if len(strTitle)>1:\n",
    "            result = db.urllib.insert_one({\n",
    "                \"title\": strTitle,\n",
    "                \"link\":strUrl,\n",
    "                \"body\":strBody,\n",
    "                \"postdate\":strDate,\n",
    "                \"datetime\":datetime.datetime.now(),\n",
    "                \"updatetime\":datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "            })\n",
    "    handle.close()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    items = []\n",
    "    start_requests();\n",
    "    row_json = json.dumps(items, ensure_ascii=False)\n",
    "    file = codecs.open(urllib.parse.unquote(keyword)+'.json', 'w', encoding='utf-8')\n",
    "    #file = codecs.open('out.json', 'w', encoding='utf-8')\n",
    "    file.write(row_json)\n",
    "    file.close()\n",
    "    #print(row_json)\n",
    "    print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
