{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用 BeautifulSoup 拆解法 - 自由時報關鍵字查詢為例 - 詳文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://news.ltn.com.tw/search?keyword=%E5%B7%9D%E6%99%AE&conditions=and&SYear=2018&SMonth=03&SDay=02&EYear=2018&EMonth=03&EDay=05&page=1\n",
      "http://news.ltn.com.tw/https://news.ltn.com.tw/news/world/breakingnews/3217950\n",
      "http://news.ltn.com.tw/https://news.ltn.com.tw/news/business/breakingnews/3217725\n",
      "http://news.ltn.com.tw/https://news.ltn.com.tw/news/world/breakingnews/3217828\n",
      "http://news.ltn.com.tw/https://news.ltn.com.tw/news/business/breakingnews/3217781\n",
      "http://news.ltn.com.tw/https://news.ltn.com.tw/news/world/breakingnews/3217660\n",
      "http://news.ltn.com.tw/https://news.ltn.com.tw/news/business/breakingnews/3217535\n",
      "http://news.ltn.com.tw/https://news.ltn.com.tw/news/world/breakingnews/3217581\n",
      "http://news.ltn.com.tw/https://news.ltn.com.tw/news/health/breakingnews/3217611\n",
      "http://news.ltn.com.tw/https://news.ltn.com.tw/news/world/breakingnews/3217510\n",
      "http://news.ltn.com.tw/https://news.ltn.com.tw/news/politics/breakingnews/3217394\n",
      "http://news.ltn.com.tw/https://news.ltn.com.tw/news/world/paper/1384073\n",
      "http://news.ltn.com.tw/https://news.ltn.com.tw/news/world/breakingnews/3217433\n",
      "http://news.ltn.com.tw/https://news.ltn.com.tw/news/politics/breakingnews/3217359\n",
      "http://news.ltn.com.tw/https://news.ltn.com.tw/news/world/breakingnews/3217218\n",
      "http://news.ltn.com.tw/https://news.ltn.com.tw/news/opinion/breakingnews/3217100\n",
      "川普國慶演講曝光！嚴詞譴責示威者「抹去歷史」\n",
      "趁川普還在位 加拿大動手建造跨國輸油管線\n",
      "否認川金會可能 北韓副外相：沒有必要\n",
      "美前財長薩默斯：政府處理疫情無能 動搖美國地位\n",
      "川普長子女友傳確診  白宮否認曾與川普接觸\n",
      "武漢肺炎敲醒澳洲！專家：將與中國「部分經濟脫鉤」\n",
      "武漢肺炎》美再創單日新高5.6萬人確診！川普仍出席國慶煙火活動\n",
      "健康網》世衛：最快2週內得知武肺藥物臨床試驗結果\n",
      "不滿美國會通過「香港自治法」 港府揚言反制\n",
      "受《德國之聲》專訪 謝志偉：全球應該制裁中國\n",
      "全球反惡法 習訪日生變\n",
      "不怕被控勾結外國勢力 民調：近6成港人挺美國制裁\n",
      "武漢肺炎》美單日確診數創全球新高 川普：這是個好消息\n",
      "中國霸凌民主盟友 波頓要澳洲硬起來\n",
      "自由限時批》中共新「統一」清洗政策\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#coding:utf-8\n",
    "#65001\n",
    "import urllib.request\n",
    "import json\n",
    "import codecs\n",
    "import sys\n",
    "import argparse as ap\n",
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "import random\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.parse import quote\n",
    "\n",
    "#python main.py 八仙塵爆 2015-06-27 2015-08-24 1\n",
    "#def argParse():\n",
    "#    parser=ap.ArgumentParser(description='Liberty Time Net Crawler')\n",
    "#    parser.add_argument(\"keyword\", help=\"Serch Keyword\")\n",
    "#    parser.add_argument(\"start_date\", help=\"Start (2017-01-01)\")\n",
    "#    parser.add_argument(\"end_date\", help=\"End (2017-01-02)\")\n",
    "#    parser.add_argument(\"pages\", help=\"Pages\")\n",
    "#    return parser.parse_args()\n",
    "\n",
    "#args=argParse()\n",
    "#keyword = quote(args.keyword)\n",
    "#start_date = args.start_date\n",
    "#end_date = args.end_date\n",
    "#pages = args.pages\n",
    "\n",
    "keyword = quote('川普')\n",
    "start_date = '2018-03-02'\n",
    "end_date = '2018-03-05'\n",
    "pages = '1'\n",
    "\n",
    "\n",
    "def start_requests():\n",
    "    if( len(start_date.split(\"-\"))==3 and len(end_date.split(\"-\"))==3) :\n",
    "        SYear = start_date.split(\"-\")[0]\n",
    "        SMonth = start_date.split(\"-\")[1]\n",
    "        SDay = start_date.split(\"-\")[2]\n",
    "        EYear = end_date.split(\"-\")[0]\n",
    "        EMonth = end_date.split(\"-\")[1]\n",
    "        EDay = end_date.split(\"-\")[2]\n",
    "        urls = []\n",
    "        for i in range(1,int(pages)+1):\n",
    "            str_idx = ''+('%s' % i)\n",
    "            urls.append('http://news.ltn.com.tw/search?keyword='+keyword+'&conditions=and&SYear='+SYear+'&SMonth='+SMonth+'&SDay='+SDay+'&EYear='+EYear+'&EMonth='+EMonth+'&EDay='+EDay+'&page='+str_idx+'')\n",
    "\n",
    "        for url in urls:\n",
    "            print (url)\n",
    "            parseLtnNews(url)\n",
    "            time.sleep(0.5)\n",
    "    else:\n",
    "        print (\"Data format error.\")\n",
    "\n",
    "\n",
    "def request_uri(uri):\n",
    "    header = {\"User-Agent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}\n",
    "    rs = requests.session()\n",
    "    res = rs.get(uri, headers=header)\n",
    "    html_data =  res.text\n",
    "    #r = requests.post(url=uri, headers={'Connection':'close'})\n",
    "    return html_data\n",
    "\n",
    "\n",
    "def parseLtnNews(uri):\n",
    "    html_data =  request_uri(uri)\n",
    "    soup = bs(html_data,'html.parser')\n",
    "    postdate = []\n",
    "    link = []\n",
    "    title = []\n",
    "    body = []\n",
    "    for ul_soup in soup.findAll('ul',attrs={\"class\":\"searchlist boxTitle\"}):\n",
    "        for span_soup in ul_soup.findAll('span'):\n",
    "            pd = span_soup.string.replace(\"&nbsp;\",\"\")[:10]\n",
    "            postdate.append(pd)\n",
    "        for a_soup in ul_soup.findAll('a',attrs={\"class\":\"tit\"}):\n",
    "            tmp_body = ''\n",
    "            tle = a_soup.getText()\n",
    "            lnk = 'http://news.ltn.com.tw/'+a_soup.get('href')\n",
    "            print(lnk)\n",
    "            title.append(tle.strip())\n",
    "            link.append(lnk)\n",
    "            html_data = request_uri(lnk)\n",
    "            soup2 = bs(html_data,'html.parser')\n",
    "            for newslistul_soup in soup2.findAll('div',attrs={\"class\":\"text\"}):\n",
    "                for p_soup in newslistul_soup.findAll('p'):\n",
    "                    tmp_body += p_soup.getText()\n",
    "            body.append(tmp_body)\n",
    "                    #items.append({\"uri\":uri,\"p_soup\":str(p_soup),\"updatetime\":datetime.datetime.now().strftime('%Y-%m-%d')})\n",
    "                    #print({\"uri\":uri,\"p_soup\":str(p_soup),\"updatetime\":datetime.datetime.now().strftime('%Y-%m-%d')})\n",
    "      \n",
    "\n",
    "    current = 0\n",
    "    while current < len(postdate):\n",
    "        print(title[current])\n",
    "        items.append({\n",
    "            \"title\": title[current],\n",
    "            \"link\":link[current],\n",
    "            \"body\":body[current],\n",
    "            \"postdate\":postdate[current],\n",
    "            #\"updatetime\":datetime.datetime.now(),  # MongoDB\n",
    "            \"updatetime\":datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "          })\n",
    "        current+=1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    items = []\n",
    "    start_requests();\n",
    "    row_json = json.dumps(items, ensure_ascii=False)\n",
    "    file = codecs.open(urllib.parse.unquote(keyword)+'.json', 'w', encoding='utf-8')\n",
    "    file.write(row_json)\n",
    "    file.close()\n",
    "  \n",
    "    print(\"Done\")\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
