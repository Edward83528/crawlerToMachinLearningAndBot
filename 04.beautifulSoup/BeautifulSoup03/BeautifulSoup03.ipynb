{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用 BeautifulSoup 拆解法 - 自由時報關鍵字查詢為例 - 詳文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://news.ltn.com.tw/search?keyword=%E5%B7%9D%E6%99%AE&conditions=and&SYear=2018&SMonth=03&SDay=02&EYear=2018&EMonth=03&EDay=05&page=1\n",
      "http://news.ltn.com.tw/news/world/breakingnews/2356598\n",
      "http://news.ltn.com.tw/news/world/breakingnews/2356586\n",
      "http://news.ltn.com.tw/news/politics/breakingnews/2355794\n",
      "http://news.ltn.com.tw/news/business/breakingnews/2355774\n",
      "http://news.ltn.com.tw/news/world/breakingnews/2355722\n",
      "http://news.ltn.com.tw/news/business/breakingnews/2355717\n",
      "http://news.ltn.com.tw/news/focus/paper/1181200\n",
      "http://news.ltn.com.tw/news/world/paper/1181182\n",
      "http://news.ltn.com.tw/news/business/paper/1181173\n",
      "http://news.ltn.com.tw/news/business/paper/1181172\n",
      "http://news.ltn.com.tw/news/business/paper/1181171\n",
      "http://news.ltn.com.tw/news/business/paper/1181170\n",
      "http://news.ltn.com.tw/news/politics/paper/1181162\n",
      "http://news.ltn.com.tw/news/opinion/paper/1181131\n",
      "http://news.ltn.com.tw/news/weeklybiz/paper/1180994\n",
      "貿易戰一觸即發 德國警告美國：勿踏上錯誤道路\n",
      "習近平延任恐成威脅  紐約時報：歐洲期待落空\n",
      "陳菊17日起訪美 20日於華府智庫演講\n",
      "3/5重要財經新聞一覽\n",
      "史上最殘酷！朝鮮半島若開戰 美軍：首日數10萬死傷\n",
      "台股盤前》台股守半年線位置 近期仍呈現整理格局\n",
      "反制中國 美航母越戰後首訪越南\n",
      "緩和兩韓關係 南韓國安特使團赴朝\n",
      "台股元宵行情落漆 短線區間震盪\n",
      "美國優先 企業憂裁員、關廠\n",
      "貿易戰恐成美股最大黑天鵝\n",
      "川普反嗆歐盟 揚言祭汽車關稅\n",
      "廢國家主席任期 人大︰有助維護習核心\n",
      "社論》務實因應美中貿易戰\n",
      "＜上週數據＞川普鋼鋁課稅計畫 美元揚升又回檔\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#coding:utf-8\n",
    "#65001\n",
    "import urllib.request\n",
    "import json\n",
    "import codecs\n",
    "import sys\n",
    "import argparse as ap\n",
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "import random\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.parse import quote\n",
    "\n",
    "#python main.py 八仙塵爆 2015-06-27 2015-08-24 1\n",
    "#def argParse():\n",
    "#    parser=ap.ArgumentParser(description='Liberty Time Net Crawler')\n",
    "#    parser.add_argument(\"keyword\", help=\"Serch Keyword\")\n",
    "#    parser.add_argument(\"start_date\", help=\"Start (2017-01-01)\")\n",
    "#    parser.add_argument(\"end_date\", help=\"End (2017-01-02)\")\n",
    "#    parser.add_argument(\"pages\", help=\"Pages\")\n",
    "#    return parser.parse_args()\n",
    "\n",
    "#args=argParse()\n",
    "#keyword = quote(args.keyword)\n",
    "#start_date = args.start_date\n",
    "#end_date = args.end_date\n",
    "#pages = args.pages\n",
    "\n",
    "keyword = quote('川普')\n",
    "start_date = '2018-03-02'\n",
    "end_date = '2018-03-05'\n",
    "pages = '1'\n",
    "\n",
    "\n",
    "def start_requests():\n",
    "    if( len(start_date.split(\"-\"))==3 and len(end_date.split(\"-\"))==3) :\n",
    "        SYear = start_date.split(\"-\")[0]\n",
    "        SMonth = start_date.split(\"-\")[1]\n",
    "        SDay = start_date.split(\"-\")[2]\n",
    "        EYear = end_date.split(\"-\")[0]\n",
    "        EMonth = end_date.split(\"-\")[1]\n",
    "        EDay = end_date.split(\"-\")[2]\n",
    "        urls = []\n",
    "        for i in range(1,int(pages)+1):\n",
    "            str_idx = ''+('%s' % i)\n",
    "            urls.append('http://news.ltn.com.tw/search?keyword='+keyword+'&conditions=and&SYear='+SYear+'&SMonth='+SMonth+'&SDay='+SDay+'&EYear='+EYear+'&EMonth='+EMonth+'&EDay='+EDay+'&page='+str_idx+'')\n",
    "\n",
    "        for url in urls:\n",
    "            print (url)\n",
    "            parseLtnNews(url)\n",
    "            time.sleep(0.5)\n",
    "    else:\n",
    "        print (\"Data format error.\")\n",
    "\n",
    "\n",
    "def request_uri(uri):\n",
    "    header = {\"User-Agent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}\n",
    "    rs = requests.session()\n",
    "    res = rs.get(uri, headers=header)\n",
    "    html_data =  res.text\n",
    "    #r = requests.post(url=uri, headers={'Connection':'close'})\n",
    "    return html_data\n",
    "\n",
    "\n",
    "def parseLtnNews(uri):\n",
    "    html_data =  request_uri(uri)\n",
    "    soup = bs(html_data,'html.parser')\n",
    "    postdate = []\n",
    "    link = []\n",
    "    title = []\n",
    "    body = []\n",
    "    for ul_soup in soup.findAll('ul',attrs={\"id\":\"newslistul\"}):\n",
    "        for span_soup in ul_soup.findAll('span'):\n",
    "            pd = span_soup.string.replace(\"&nbsp;\",\"\")[:10]\n",
    "            postdate.append(pd)\n",
    "        for a_soup in ul_soup.findAll('a',attrs={\"class\":\"tit\"}):\n",
    "            tmp_body = ''\n",
    "            tle = a_soup.getText()\n",
    "            lnk = 'http://news.ltn.com.tw/'+a_soup.get('href')\n",
    "            print(lnk)\n",
    "            title.append(tle.strip())\n",
    "            link.append(lnk)\n",
    "            html_data = request_uri(lnk)\n",
    "            soup2 = bs(html_data,'html.parser')\n",
    "            for newslistul_soup in soup2.findAll('div',attrs={\"class\":\"text\"}):\n",
    "                for p_soup in newslistul_soup.findAll('p'):\n",
    "                    tmp_body += p_soup.getText()\n",
    "            body.append(tmp_body)\n",
    "                    #items.append({\"uri\":uri,\"p_soup\":str(p_soup),\"updatetime\":datetime.datetime.now().strftime('%Y-%m-%d')})\n",
    "                    #print({\"uri\":uri,\"p_soup\":str(p_soup),\"updatetime\":datetime.datetime.now().strftime('%Y-%m-%d')})\n",
    "      \n",
    "\n",
    "    current = 0\n",
    "    while current < len(postdate):\n",
    "        print(title[current])\n",
    "        items.append({\n",
    "            \"title\": title[current],\n",
    "            \"link\":link[current],\n",
    "            \"body\":body[current],\n",
    "            \"postdate\":postdate[current],\n",
    "            #\"updatetime\":datetime.datetime.now(),  # MongoDB\n",
    "            \"updatetime\":datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "          })\n",
    "        current+=1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    items = []\n",
    "    start_requests();\n",
    "    row_json = json.dumps(items, ensure_ascii=False)\n",
    "    file = codecs.open(urllib.parse.unquote(keyword)+'.json', 'w', encoding='utf-8')\n",
    "    file.write(row_json)\n",
    "    file.close()\n",
    "  \n",
    "    print(\"Done\")\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
