{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用 BeautifulSoup 拆解法 - 自由時報關鍵字查詢為例 - 簡要內文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://news.ltn.com.tw/search?keyword=%E8%95%AD%E6%95%AC%E9%A8%B0&conditions=and&SYear=2020&SMonth=06&SDay=01&EYear=2020&EMonth=06&EDay=30&page=1&rand=60971\n",
      "15\n",
      "老蕭子弟兵搶先加盟環球 「超派」林孟辰撩妹變小綿羊\n",
      "超派！蕭敬騰愛徒加盟環球 林孟辰慘遭砍20首歌\n",
      "張清芳現身了！阿妹經紀人結婚晚宴獻唱這首歌\n",
      "張孟權樂獲老蕭讚賞   害羞不敢揪偶像合照\n",
      "Sandro Homme登台 蕭敬騰搶頭香\n",
      "跟風自PO「性轉照」 連勝文自嘲：我還是當男生好了\n",
      "火辣！混血辣模只穿丁字褲    蕭敬騰上陣「挑逗」\n",
      "蕭敬騰執導MV 逼小強半裸挑逗混血女模\n",
      "蕭敬騰變了！竟要愛徒半裸 「挑逗」混血辣模\n",
      "神準！蕭敬騰拍片預測大樂透    粉絲一買中獎了\n",
      "扯爆了！蕭敬騰「廢片」預測大樂透    同事歌迷竟中獎\n",
      "向太怒了！心疼媳婦受委屈 嗆聲派「老蕭的狗」咬人\n",
      "蕭敬騰拍MV帶頭衝 自導自演24小時未闔眼\n",
      "蕭敬騰遭關在籠中 24小時未闔眼奔向自由\n",
      "（專訪）窮到「剩下2塊錢」！男星巧遇瑪莎結局超展開\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#coding:utf-8\n",
    "#65001\n",
    "import urllib.request\n",
    "import json\n",
    "import codecs\n",
    "import sys\n",
    "import argparse as ap\n",
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "import random\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.parse import quote\n",
    "\n",
    "#python main.py 八仙塵爆 2015-06-27 2015-08-24 1\n",
    "#def argParse():\n",
    "#    parser=ap.ArgumentParser(description='Liberty Time Net Crawler')\n",
    "#    parser.add_argument(\"keyword\", help=\"Serch Keyword\")\n",
    "#    parser.add_argument(\"start_date\", help=\"Start (2017-01-01)\")\n",
    "#    parser.add_argument(\"end_date\", help=\"End (2017-01-02)\")\n",
    "#    parser.add_argument(\"pages\", help=\"Pages\")\n",
    "#    return parser.parse_args()\n",
    "\n",
    "#args=argParse()\n",
    "#keyword = quote(args.keyword)\n",
    "#start_date = args.start_date\n",
    "#end_date = args.end_date\n",
    "#pages = args.pages\n",
    "\n",
    "\n",
    "keyword = quote('蕭敬騰')\n",
    "start_date = '2020-06-01'\n",
    "end_date = '2020-06-30'\n",
    "pages = '1'\n",
    "\n",
    "def start_requests():\n",
    "    if( len(start_date.split(\"-\"))==3 and len(end_date.split(\"-\"))==3) :\n",
    "        SYear = start_date.split(\"-\")[0]\n",
    "        SMonth = start_date.split(\"-\")[1]\n",
    "        SDay = start_date.split(\"-\")[2]\n",
    "        EYear = end_date.split(\"-\")[0]\n",
    "        EMonth = end_date.split(\"-\")[1]\n",
    "        EDay = end_date.split(\"-\")[2]\n",
    "\n",
    "        urls = []\n",
    "        for i in range(1,int(pages)+1):\n",
    "            str_idx = ''+('%s' % i)\n",
    "            str_rand = str(random.randint(1,99999))\n",
    "            urls.append('http://news.ltn.com.tw/search?keyword='+keyword+'&conditions=and&SYear='+SYear+'&SMonth='+SMonth+'&SDay='+SDay+'&EYear='+EYear+'&EMonth='+EMonth+'&EDay='+EDay+'&page='+str_idx+'&rand='+str_rand+'')\n",
    "\n",
    "        for url in urls:\n",
    "            print (url)\n",
    "            parseLtnNews(url)\n",
    "            time.sleep(0.5)\n",
    "    else:\n",
    "        print (\"Data format error.\")\n",
    "\n",
    "\n",
    "def request_uri(uri):\n",
    "    header = {\"User-Agent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}\n",
    "    rs = requests.session()\n",
    "    res = rs.get(uri, headers=header)\n",
    "    html_data =  res.text\n",
    "    #r = requests.get(url=uri, headers={'Connection':'close'})\n",
    "    return html_data\n",
    "\n",
    "\n",
    "def parseLtnNews(uri):\n",
    "    html_data =  request_uri(uri)\n",
    "    soup = bs(html_data,'html.parser')\n",
    "    postdate = []\n",
    "    link = []\n",
    "    title = []\n",
    "    body = []\n",
    "    for ul_soup in soup.findAll('ul',attrs={\"class\":\"searchlist boxTitle\"}):\n",
    "        #print(ul_soup)\n",
    "        for span_soup in ul_soup.findAll('span'):\n",
    "            pd = span_soup.string.replace(\"&nbsp;\",\"\")[:10]\n",
    "            postdate.append(pd)\n",
    "        for li_soup in ul_soup.findAll('li'):\n",
    "            p_list = li_soup.findAll('p')\n",
    "            body.append(p_list[0].getText())\n",
    "        for a_soup in ul_soup.findAll('a',attrs={\"class\":\"tit\"}):\n",
    "            tle = a_soup.getText()\n",
    "            lnk = 'http://news.ltn.com.tw/'+a_soup.get('href')\n",
    "            title.append(tle.strip())\n",
    "            link.append(lnk)\n",
    "\n",
    "    current = 0\n",
    "    print(str(len(postdate)))\n",
    "    while current < len(postdate):\n",
    "        print(title[current])\n",
    "        items.append({\n",
    "              \"title\": title[current],\n",
    "              \"link\":link[current],\n",
    "              \"body\":body[current],\n",
    "              \"postdate\":postdate[current],\n",
    "              #\"updatetime\":datetime.datetime.now(),  # MongoDB\n",
    "              \"updatetime\":datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "          })\n",
    "        current+=1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    items = []\n",
    "    start_requests();\n",
    "    row_json = json.dumps(items, ensure_ascii=False)\n",
    "    file = codecs.open(urllib.parse.unquote(keyword)+'.json', 'w', encoding='utf-8')\n",
    "    file.write(row_json)\n",
    "    file.close()\n",
    "  \n",
    "    print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
